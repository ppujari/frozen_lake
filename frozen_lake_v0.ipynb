{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "## Project: Frozen Lake\n",
    "\n",
    "The Frozen Lake environment is a 4×4 grid which contain four possible features  — Safe (S), Frozen (F), Hole (H) and Goal (G). The agent moves around the grid until it reaches the goal or hole. If it falls into the hole, it has to start from the beginning and is rewarded the value 0. The process continues until it learns from every mistake and eventually reaches the goal . Here is visual description of the Frozen Lake grid (4×4):\n",
    "\n",
    "![](Frozen-Lake.png)\n",
    "\n",
    "       ( This picture is collected from web )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Step 0: Getting Started\n",
    "It can go left, right, up and down.\n",
    "\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "Examine the State and Action Spaces\n",
    "At each time step, it has four actions at its disposal:\n",
    "\n",
    "0 - up\n",
    "1 - down\n",
    "2 - left\n",
    "3 - right\n",
    "\n",
    "![](reinforcement-learning-fig1-700.jpg)\n",
    "\n",
    "                                General RL topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "# Pretty display for notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create FrozenLake environment\n",
    "* Here we'll create the FrozenLake environment.\n",
    "* OpenAI Gym is a library provides many environments that we can use to train our agents.\n",
    "* In our case we choose to use Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create environment\n",
    "env = gym.make('FrozenLake-v0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Create Q-Table and initialize it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "States have length: 16\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "print('States have length:', state_size)\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: hyper param values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100000            # Total episodes\n",
    "max_steps_per_episodes = 100    # Max steps per episode\n",
    "learning_rate = 0.7             # Learning rate\n",
    "discount_rate = 0.99            # Discounting rate\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001  # Exponential decay rate for exploration\n",
    "print_every = 1000              # Print rewards at every 1000 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: 0.00\n",
      "Episode 1000\tAverage Score: 0.03\n",
      "Episode 2000\tAverage Score: 0.06\n",
      "Episode 3000\tAverage Score: 0.11\n",
      "Episode 4000\tAverage Score: 0.17\n",
      "Episode 5000\tAverage Score: 0.23\n",
      "Episode 6000\tAverage Score: 0.28\n",
      "Episode 7000\tAverage Score: 0.32\n",
      "Episode 8000\tAverage Score: 0.34\n",
      "Episode 9000\tAverage Score: 0.37\n",
      "Episode 10000\tAverage Score: 0.39\n",
      "Episode 11000\tAverage Score: 0.40\n",
      "Episode 12000\tAverage Score: 0.41\n",
      "Episode 13000\tAverage Score: 0.42\n",
      "Episode 14000\tAverage Score: 0.43\n",
      "Episode 15000\tAverage Score: 0.43\n",
      "Episode 16000\tAverage Score: 0.44\n",
      "Episode 17000\tAverage Score: 0.44\n",
      "Episode 18000\tAverage Score: 0.45\n",
      "Episode 19000\tAverage Score: 0.45\n",
      "Episode 20000\tAverage Score: 0.45\n",
      "Episode 21000\tAverage Score: 0.46\n",
      "Episode 22000\tAverage Score: 0.46\n",
      "Episode 23000\tAverage Score: 0.47\n",
      "Episode 24000\tAverage Score: 0.47\n",
      "Episode 25000\tAverage Score: 0.47\n",
      "Episode 26000\tAverage Score: 0.47\n",
      "Episode 27000\tAverage Score: 0.47\n",
      "Episode 28000\tAverage Score: 0.48\n",
      "Episode 29000\tAverage Score: 0.48\n",
      "Episode 30000\tAverage Score: 0.48\n",
      "Episode 31000\tAverage Score: 0.48\n",
      "Episode 32000\tAverage Score: 0.48\n",
      "Episode 33000\tAverage Score: 0.48\n",
      "Episode 34000\tAverage Score: 0.48\n",
      "Episode 35000\tAverage Score: 0.48\n",
      "Episode 36000\tAverage Score: 0.49\n",
      "Episode 37000\tAverage Score: 0.49\n",
      "Episode 38000\tAverage Score: 0.49\n",
      "Episode 39000\tAverage Score: 0.49\n",
      "Episode 40000\tAverage Score: 0.49\n",
      "Episode 41000\tAverage Score: 0.49\n",
      "Episode 42000\tAverage Score: 0.50\n",
      "Episode 43000\tAverage Score: 0.50\n",
      "Episode 44000\tAverage Score: 0.50\n",
      "Episode 45000\tAverage Score: 0.50\n",
      "Episode 46000\tAverage Score: 0.50\n",
      "Episode 47000\tAverage Score: 0.50\n",
      "Episode 48000\tAverage Score: 0.50\n",
      "Episode 49000\tAverage Score: 0.50\n",
      "Episode 50000\tAverage Score: 0.50\n",
      "Episode 51000\tAverage Score: 0.50\n",
      "Episode 52000\tAverage Score: 0.50\n",
      "Episode 53000\tAverage Score: 0.50\n",
      "Episode 54000\tAverage Score: 0.50\n",
      "Episode 55000\tAverage Score: 0.50\n",
      "Episode 56000\tAverage Score: 0.50\n",
      "Episode 57000\tAverage Score: 0.51\n",
      "Episode 58000\tAverage Score: 0.51\n",
      "Episode 59000\tAverage Score: 0.51\n",
      "Episode 60000\tAverage Score: 0.51\n",
      "Episode 61000\tAverage Score: 0.51\n",
      "Episode 62000\tAverage Score: 0.51\n",
      "Episode 63000\tAverage Score: 0.51\n",
      "Episode 64000\tAverage Score: 0.51\n",
      "Episode 65000\tAverage Score: 0.51\n",
      "Episode 66000\tAverage Score: 0.51\n",
      "Episode 67000\tAverage Score: 0.51\n",
      "Episode 68000\tAverage Score: 0.51\n",
      "Episode 69000\tAverage Score: 0.51\n",
      "Episode 70000\tAverage Score: 0.51\n",
      "Episode 71000\tAverage Score: 0.51\n",
      "Episode 72000\tAverage Score: 0.51\n",
      "Episode 73000\tAverage Score: 0.51\n",
      "Episode 74000\tAverage Score: 0.51\n",
      "Episode 75000\tAverage Score: 0.52\n",
      "Episode 76000\tAverage Score: 0.52\n",
      "Episode 77000\tAverage Score: 0.52\n",
      "Episode 78000\tAverage Score: 0.52\n",
      "Episode 79000\tAverage Score: 0.52\n",
      "Episode 80000\tAverage Score: 0.52\n",
      "Episode 81000\tAverage Score: 0.52\n",
      "Episode 82000\tAverage Score: 0.52\n",
      "Episode 83000\tAverage Score: 0.52\n",
      "Episode 84000\tAverage Score: 0.52\n",
      "Episode 85000\tAverage Score: 0.52\n",
      "Episode 86000\tAverage Score: 0.52\n",
      "Episode 87000\tAverage Score: 0.52\n",
      "Episode 88000\tAverage Score: 0.52\n",
      "Episode 89000\tAverage Score: 0.52\n",
      "Episode 90000\tAverage Score: 0.52\n",
      "Episode 91000\tAverage Score: 0.52\n",
      "Episode 92000\tAverage Score: 0.52\n",
      "Episode 93000\tAverage Score: 0.52\n",
      "Episode 94000\tAverage Score: 0.52\n",
      "Episode 95000\tAverage Score: 0.52\n",
      "Episode 96000\tAverage Score: 0.52\n",
      "Episode 97000\tAverage Score: 0.52\n",
      "Episode 98000\tAverage Score: 0.52\n",
      "Episode 99000\tAverage Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "# Q-Learning algo\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0;\n",
    "    for step in range( max_steps_per_episodes):\n",
    "        #env.render()\n",
    "#    while True:\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "#update q table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state,action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[next_state,:]))\n",
    "  #          Q[s,a] = Q[s,a] + eta*(r + gma*np.max(Q[s1,:]) - Q[s,a])\n",
    "\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    rewards.append(score) \n",
    "    #env.render()\n",
    "\n",
    "    if episode % print_every == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(rewards)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Sum on all episodes 0.5205\n",
      "********** Q TABLE ********\n",
      "\n",
      "[[5.54154239e-01 4.97649148e-01 4.64100062e-01 4.88555828e-01]\n",
      " [2.15800202e-01 2.34385806e-01 4.04414024e-01 5.27415966e-01]\n",
      " [2.01403059e-01 2.12287945e-01 1.96462070e-01 4.18618177e-01]\n",
      " [3.61642154e-01 2.62079092e-02 2.20349259e-01 4.50843276e-01]\n",
      " [6.17218608e-01 4.67944197e-01 4.63533141e-02 4.57858729e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.38526839e-04 1.80083255e-03 6.12199557e-01 1.18684725e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.29112362e-03 4.85963142e-02 1.26587468e-02 7.55289283e-01]\n",
      " [5.09321487e-03 8.40417574e-01 5.29077102e-02 2.08553460e-02]\n",
      " [8.91495603e-01 1.86296952e-02 1.52039850e-02 7.87373730e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.90193720e-01 8.40550436e-02 9.26857529e-01 2.05255108e-01]\n",
      " [6.32972283e-01 9.87600904e-01 5.58811661e-01 6.12722378e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Reward Sum on all episodes \" + str(sum(rewards)/num_episodes))\n",
    "\n",
    "#Print updated Q table\n",
    "print( \"********** Q TABLE ********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use Q-Table to play "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 14\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 58\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 19\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 11\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 21\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps_per_episodes):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Q-Learning: DQN ( Deep Q LEarning )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Q Learning, table size depends on the number of states and actions ( m x n ).  For a large m or n, Q-table potentially be a scalability problem.**\n",
    "\n",
    "Deep Q Learning algorithm represents the optimal action-value function q, as a neural network instead of a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import additianl packeages\n",
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Number of steps 2\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Number of steps 3\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Number of steps 19\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Number of steps 32\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Number of steps 47\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Number of steps 4\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Number of steps 44\n"
     ]
    }
   ],
   "source": [
    "from dqn_agent import Agent\n",
    "from torch.autograd import Variable\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "# watch an untrained agent\n",
    "\n",
    "for i in range(7):\n",
    "    state = env.reset()  # state is a int number corresponding to the agent's position in a board\n",
    "\n",
    "    for j in range(500):\n",
    "        curr_state_encoded = np.eye(state_size)[state].reshape(1,state_size)\n",
    "        action = agent.act(curr_state_encoded)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "            print(\"Number of steps\", j)\n",
    "            break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(size,value):\n",
    "    \"\"\"1 hot encoding for observed state\"\"\"\n",
    "    return np.eye(size)[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=3000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            curr_state_encoded = to_onehot(state_size, state).reshape(1,state_size)\n",
    "            #print ( curr_state_encoded.shape )\n",
    "            action = agent.act(curr_state_encoded, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state_encoded = to_onehot(state_size, next_state).reshape(1, state_size)\n",
    "            agent.step(curr_state_encoded, action, reward, next_state_encoded, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "    \n",
    "    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        \n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.01\n",
      "Episode 200\tAverage Score: 0.01\n",
      "Episode 300\tAverage Score: 0.11\n",
      "Episode 400\tAverage Score: 0.10\n",
      "Episode 500\tAverage Score: 0.24\n",
      "Episode 600\tAverage Score: 0.23\n",
      "Episode 700\tAverage Score: 0.31\n",
      "Episode 800\tAverage Score: 0.42\n",
      "Episode 900\tAverage Score: 0.47\n",
      "Episode 1000\tAverage Score: 0.42\n",
      "Episode 1100\tAverage Score: 0.44\n",
      "Episode 1200\tAverage Score: 0.53\n",
      "Episode 1300\tAverage Score: 0.47\n",
      "Episode 1400\tAverage Score: 0.60\n",
      "Episode 1500\tAverage Score: 0.44\n",
      "Episode 1600\tAverage Score: 0.49\n",
      "Episode 1700\tAverage Score: 0.58\n",
      "Episode 1800\tAverage Score: 0.57\n",
      "Episode 1900\tAverage Score: 0.50\n",
      "Episode 2000\tAverage Score: 0.55\n",
      "Episode 2100\tAverage Score: 0.56\n",
      "Episode 2200\tAverage Score: 0.53\n",
      "Episode 2300\tAverage Score: 0.63\n",
      "Episode 2400\tAverage Score: 0.68\n",
      "Episode 2500\tAverage Score: 0.55\n",
      "Episode 2600\tAverage Score: 0.72\n",
      "Episode 2700\tAverage Score: 0.62\n",
      "Episode 2800\tAverage Score: 0.60\n",
      "Episode 2900\tAverage Score: 0.61\n",
      "Episode 3000\tAverage Score: 0.61\n",
      "Episode 3100\tAverage Score: 0.58\n",
      "Episode 3200\tAverage Score: 0.64\n",
      "Episode 3300\tAverage Score: 0.66\n",
      "Episode 3400\tAverage Score: 0.61\n",
      "Episode 3500\tAverage Score: 0.68\n",
      "Episode 3600\tAverage Score: 0.59\n",
      "Episode 3700\tAverage Score: 0.72\n",
      "Episode 3800\tAverage Score: 0.72\n",
      "Episode 3900\tAverage Score: 0.63\n",
      "Episode 4000\tAverage Score: 0.68\n",
      "Episode 4100\tAverage Score: 0.61\n",
      "Episode 4200\tAverage Score: 0.64\n",
      "Episode 4300\tAverage Score: 0.66\n",
      "Episode 4400\tAverage Score: 0.67\n",
      "Episode 4500\tAverage Score: 0.65\n",
      "Episode 4600\tAverage Score: 0.70\n",
      "Episode 4700\tAverage Score: 0.71\n",
      "Episode 4800\tAverage Score: 0.69\n",
      "Episode 4900\tAverage Score: 0.66\n",
      "Episode 5000\tAverage Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "scores = dqn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch a Smart Agent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 79\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 51\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 23\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 78\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Number of steps 69\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 24\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Number of steps 18\n"
     ]
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(7):\n",
    "    state = env.reset()\n",
    "\n",
    "    for j in range(500):\n",
    "        curr_state_encoded = np.eye(state_size)[state].reshape(1,state_size)\n",
    "        action = agent.act(curr_state_encoded)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "            print(\"Number of steps\", j)\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should try to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
